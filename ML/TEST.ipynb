{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\mshome\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (4.37.0)\n",
      "Requirement already satisfied: torchvision in c:\\users\\mshome\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (0.16.2)\n",
      "Requirement already satisfied: imagehash in c:\\users\\mshome\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (4.3.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in c:\\users\\mshome\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (0.20.2)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in c:\\users\\mshome\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (0.4.1)\n",
      "Requirement already satisfied: requests in c:\\users\\mshome\\appdata\\roaming\\python\\python310\\site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\mshome\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (2023.12.25)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\mshome\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (4.66.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\mshome\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\mshome\\appdata\\roaming\\python\\python310\\site-packages (from transformers) (1.26.0)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in c:\\users\\mshome\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (0.15.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\mshome\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\mshome\\appdata\\roaming\\python\\python310\\site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\mshome\\appdata\\roaming\\python\\python310\\site-packages (from torchvision) (10.0.1)\n",
      "Requirement already satisfied: torch==2.1.2 in c:\\users\\mshome\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torchvision) (2.1.2)\n",
      "Requirement already satisfied: sympy in c:\\users\\mshome\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch==2.1.2->torchvision) (1.12)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\mshome\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch==2.1.2->torchvision) (3.1.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\mshome\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch==2.1.2->torchvision) (3.2.1)\n",
      "Requirement already satisfied: fsspec in c:\\users\\mshome\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch==2.1.2->torchvision) (2023.10.0)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\mshome\\appdata\\roaming\\python\\python310\\site-packages (from torch==2.1.2->torchvision) (4.8.0)\n",
      "Requirement already satisfied: PyWavelets in c:\\users\\mshome\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from imagehash) (1.5.0)\n",
      "Requirement already satisfied: scipy in c:\\users\\mshome\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from imagehash) (1.11.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\mshome\\appdata\\roaming\\python\\python310\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\mshome\\appdata\\roaming\\python\\python310\\site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\mshome\\appdata\\roaming\\python\\python310\\site-packages (from requests->transformers) (3.2.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\mshome\\appdata\\roaming\\python\\python310\\site-packages (from requests->transformers) (2023.7.22)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\mshome\\appdata\\roaming\\python\\python310\\site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\mshome\\appdata\\roaming\\python\\python310\\site-packages (from jinja2->torch==2.1.2->torchvision) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\mshome\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sympy->torch==2.1.2->torchvision) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.2.2 -> 23.3.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install transformers torchvision imagehash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from fpdf import FPDF\n",
    "from moviepy.editor import VideoFileClip\n",
    "from transformers import ViTFeatureExtractor, ViTForImageClassification\n",
    "import torch\n",
    "from imagehash import phash\n",
    "import os\n",
    "\n",
    "\n",
    "text_feature_extractor = ViTFeatureExtractor.from_pretrained(\"JuanMa360/text-in-image-detection\")\n",
    "text_model = ViTForImageClassification.from_pretrained(\"JuanMa360/text-in-image-detection\")\n",
    "\n",
    "\n",
    "\n",
    "scene_model = None\n",
    "\n",
    "def compute_text_probability(frame):\n",
    "    inputs_text = text_feature_extractor(images=frame, return_tensors=\"pt\")\n",
    "    outputs_text = text_model(**inputs_text)\n",
    "    logits_text = outputs_text.logits\n",
    "    probability_text = torch.nn.functional.softmax(logits_text, dim=1)[0, 2].item()\n",
    "    return probability_text\n",
    "\n",
    "def hash_image(image):\n",
    "    return phash(image)\n",
    "\n",
    "def classify_frames(frames, confidence_threshold_text=0.985, hash_threshold=10, confidence_threshold_scene=0.5, similarity_threshold=0.5):\n",
    "    processed_frames = set()\n",
    "    important_frames = []\n",
    "\n",
    "\n",
    "    for i, frame in enumerate(frames):\n",
    "\n",
    "        probability_text = compute_text_probability(frame)\n",
    "\n",
    "\n",
    "        if probability_text > confidence_threshold_text:\n",
    "            important_frames.append(frame)\n",
    "            image_hash = hash_image(frame)\n",
    "            processed_frames.add(image_hash)\n",
    "\n",
    "\n",
    "    for i in range(1, len(important_frames)):\n",
    "        current_frame = important_frames[i]\n",
    "        previous_frame = important_frames[i - 1]\n",
    "\n",
    "        similarity = 1.0 - (phash(current_frame) - phash(previous_frame)) / 64.0\n",
    "\n",
    "\n",
    "        if similarity < similarity_threshold:\n",
    "\n",
    "            important_frames.append(current_frame)\n",
    "\n",
    "\n",
    "    for frame in important_frames:\n",
    "        image_hash = hash_image(frame)\n",
    "\n",
    "\n",
    "        scene_probability = 0.0\n",
    "        if scene_model is not None:\n",
    "\n",
    "            scene_probability = 0.0\n",
    "\n",
    "        if scene_probability > confidence_threshold_scene:\n",
    "            processed_frames.add(image_hash)\n",
    "\n",
    "    return important_frames\n",
    "\n",
    "def extract_frames(video_path, num_frames=5):\n",
    "    clip = VideoFileClip(video_path)\n",
    "    frames = []\n",
    "\n",
    "    frame_interval = max(int(clip.fps * clip.duration) // num_frames, 5)\n",
    "\n",
    "    for i in range(0, int(clip.fps * clip.duration), frame_interval):\n",
    "        frame = clip.get_frame(i / clip.fps)\n",
    "        pil_image = Image.fromarray(frame.astype('uint8'), mode='RGB')\n",
    "        frames.append(pil_image)\n",
    "\n",
    "    return frames\n",
    "\n",
    "def generate_pdf(frames, output_pdf_path):\n",
    "    pdf = FPDF()\n",
    "    pdf.set_auto_page_break(auto=True, margin=15)\n",
    "    pdf.set_font(\"Arial\", size=12)\n",
    "\n",
    "    for i in range(0, len(frames), 2):\n",
    "\n",
    "        pdf.add_page()\n",
    "\n",
    "\n",
    "        image_filename_1 = f\"frame_{i + 1}.png\"\n",
    "        frames[i].save(image_filename_1, format='PNG')\n",
    "\n",
    "        available_width = pdf.w - 30  # 15 units margin on both sides\n",
    "        available_height = pdf.h - 30  # 15 units margin on both top and bottom\n",
    "\n",
    "        # Calculate the aspect ratio of the images\n",
    "        aspect_ratio_1 = frames[i].width / frames[i].height\n",
    "\n",
    "        # Calculate the width and height of the first image to fit the available space\n",
    "        width_1 = min(available_width, frames[i].width)\n",
    "        height_1 = width_1 / aspect_ratio_1\n",
    "\n",
    "        # Calculate the x and y positions for the first image\n",
    "        x_1 = 15  # Left margin\n",
    "        y_1 = 15  # Top margin\n",
    "\n",
    "        # Add the first image\n",
    "        pdf.image(image_filename_1, x=x_1, y=y_1, w=width_1, h=height_1)\n",
    "        os.remove(image_filename_1)\n",
    "\n",
    "        # Add the second image if available\n",
    "        if i + 1 < len(frames):\n",
    "            image_filename_2 = f\"frame_{i + 2}.png\"\n",
    "            frames[i + 1].save(image_filename_2, format='PNG')\n",
    "\n",
    "            # Calculate the width and height of the second image to fit the available space\n",
    "            width_2 = min(available_width, frames[i + 1].width)\n",
    "            height_2 = width_2 / aspect_ratio_1  # Use the same aspect ratio as the first image\n",
    "\n",
    "            # Calculate the x and y positions for the second image\n",
    "            x_2 = 15  # Left margin\n",
    "            y_2 = y_1 + height_1  # Place the second image below the first\n",
    "\n",
    "            # Add the second image\n",
    "            pdf.image(image_filename_2, x=x_2, y=y_2, w=width_2, h=height_2)\n",
    "            os.remove(image_filename_2)\n",
    "\n",
    "    pdf.output(output_pdf_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDF generated successfully at pdf.pdf\n"
     ]
    }
   ],
   "source": [
    "video_path = r\"video.mp4\"\n",
    "output_pdf_path = 'pdf.pdf'\n",
    "\n",
    "frames = extract_frames(video_path, num_frames=20)\n",
    "important_frames = classify_frames(frames)\n",
    "\n",
    "if important_frames:\n",
    "    generate_pdf(important_frames, output_pdf_path)\n",
    "    print(f\"PDF generated successfully at {output_pdf_path}\")\n",
    "else:\n",
    "    print(\"No important frames found.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
