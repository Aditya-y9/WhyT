{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kasodeep/inheritance-project/blob/main/llama_implementation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XoHUqH87mf5H",
        "outputId": "cc1d0745-b120-4a94-81cb-ced363554db1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tiktoken in c:\\users\\mshome\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (0.5.2)\n",
            "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\mshome\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tiktoken) (2023.12.25)\n",
            "Requirement already satisfied: requests>=2.26.0 in c:\\users\\mshome\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tiktoken) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\mshome\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests>=2.26.0->tiktoken) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\mshome\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests>=2.26.0->tiktoken) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\mshome\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests>=2.26.0->tiktoken) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\mshome\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests>=2.26.0->tiktoken) (2023.7.22)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3pH36pjU31gl",
        "outputId": "458ce18b-cf05-4301-f64b-1a947f543b3a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: langchain in c:\\users\\mshome\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (0.1.0)\n",
            "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\mshome\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from langchain) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\users\\mshome\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from langchain) (2.0.25)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\users\\mshome\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from langchain) (3.8.5)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in c:\\users\\mshome\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from langchain) (0.6.3)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\mshome\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from langchain) (1.33)\n",
            "Requirement already satisfied: langchain-community<0.1,>=0.0.9 in c:\\users\\mshome\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from langchain) (0.0.12)\n",
            "Requirement already satisfied: langchain-core<0.2,>=0.1.7 in c:\\users\\mshome\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from langchain) (0.1.10)\n",
            "Requirement already satisfied: langsmith<0.1.0,>=0.0.77 in c:\\users\\mshome\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from langchain) (0.0.80)\n",
            "Requirement already satisfied: numpy<2,>=1 in c:\\users\\mshome\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from langchain) (1.25.2)\n",
            "Requirement already satisfied: pydantic<3,>=1 in c:\\users\\mshome\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from langchain) (2.5.1)\n",
            "Requirement already satisfied: requests<3,>=2 in c:\\users\\mshome\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from langchain) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in c:\\users\\mshome\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from langchain) (8.2.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\mshome\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in c:\\users\\mshome\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (3.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\mshome\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\users\\mshome\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\mshome\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\mshome\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\mshome\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\mshome\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (3.20.2)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in c:\\users\\mshome\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (0.9.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\mshome\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from jsonpatch<2.0,>=1.33->langchain) (2.4)\n",
            "Requirement already satisfied: anyio<5,>=3 in c:\\users\\mshome\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from langchain-core<0.2,>=0.1.7->langchain) (4.2.0)\n",
            "Requirement already satisfied: packaging<24.0,>=23.2 in c:\\users\\mshome\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from langchain-core<0.2,>=0.1.7->langchain) (23.2)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\mshome\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pydantic<3,>=1->langchain) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.14.3 in c:\\users\\mshome\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pydantic<3,>=1->langchain) (2.14.3)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in c:\\users\\mshome\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pydantic<3,>=1->langchain) (4.8.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\mshome\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests<3,>=2->langchain) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\mshome\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests<3,>=2->langchain) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\mshome\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests<3,>=2->langchain) (2023.7.22)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\mshome\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\n",
            "Requirement already satisfied: sniffio>=1.1 in c:\\users\\mshome\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1.7->langchain) (1.3.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\mshome\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain) (1.0.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Requirement already satisfied: fpdf in c:\\users\\mshome\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (1.7.2)\n",
            "Requirement already satisfied: ctransformers in c:\\users\\mshome\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (0.2.27)\n",
            "Requirement already satisfied: huggingface-hub in c:\\users\\mshome\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from ctransformers) (0.20.2)\n",
            "Requirement already satisfied: py-cpuinfo<10.0.0,>=9.0.0 in c:\\users\\mshome\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from ctransformers) (9.0.0)\n",
            "Requirement already satisfied: filelock in c:\\users\\mshome\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from huggingface-hub->ctransformers) (3.12.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\mshome\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from huggingface-hub->ctransformers) (2023.9.2)\n",
            "Requirement already satisfied: requests in c:\\users\\mshome\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from huggingface-hub->ctransformers) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\mshome\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from huggingface-hub->ctransformers) (4.66.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\mshome\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from huggingface-hub->ctransformers) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\mshome\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from huggingface-hub->ctransformers) (4.8.0)\n",
            "Requirement already satisfied: packaging>=20.9 in c:\\users\\mshome\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from huggingface-hub->ctransformers) (23.2)\n",
            "Requirement already satisfied: colorama in c:\\users\\mshome\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tqdm>=4.42.1->huggingface-hub->ctransformers) (0.4.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\mshome\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests->huggingface-hub->ctransformers) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\mshome\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests->huggingface-hub->ctransformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\mshome\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests->huggingface-hub->ctransformers) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\mshome\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests->huggingface-hub->ctransformers) (2023.7.22)\n",
            "Requirement already satisfied: pypdf in c:\\users\\mshome\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (4.0.0)\n",
            "Requirement already satisfied: PyPDF2 in c:\\users\\mshome\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (3.0.1)\n"
          ]
        }
      ],
      "source": [
        "%pip install langchain\n",
        "!pip install fpdf\n",
        "!pip install ctransformers\n",
        "!pip install pypdf\n",
        "!pip install PyPDF2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "r9-cND-36Lat"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\MSHOME\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "#imports\n",
        "import langchain\n",
        "from torch import cuda, bfloat16\n",
        "from fpdf import FPDF\n",
        "import torch\n",
        "import transformers\n",
        "from transformers import AutoTokenizer\n",
        "from time import time\n",
        "from langchain.llms import HuggingFacePipeline\n",
        "from langchain.document_loaders import TextLoader,PyPDFLoader, DirectoryLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.chains import MapReduceDocumentsChain, ReduceDocumentsChain,ConversationalRetrievalChain,StuffDocumentsChain\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.llms import CTransformers\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
        "from langchain import PromptTemplate, LLMChain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Rc5epbmc8kIz"
      },
      "outputs": [
        {
          "ename": "ValidationError",
          "evalue": "1 validation error for CTransformers\n__root__\n  Model path '/content/drive/MyDrive/llama-2-13b.ggmlv3.q2_K.bin' doesn't exist. (type=value_error)",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mValidationError\u001b[0m                           Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[4], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#loading downloaded llm suitable for local usage,temperature(entropy/randomness in answer):1e-2\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# But first you would need to mount your Google Drive to Google Colab.\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m llm \u001b[38;5;241m=\u001b[39m \u001b[43mCTransformers\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/content/drive/MyDrive/llama-2-13b.ggmlv3.q2_K.bin\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mllama\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstreaming\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mStreamingStdOutCallbackHandler\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmax_new_tokens\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m4096\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtemperature\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcontext_length\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m4096\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\langchain_core\\load\\serializable.py:107\u001b[0m, in \u001b[0;36mSerializable.__init__\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    106\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 107\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    108\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lc_kwargs \u001b[38;5;241m=\u001b[39m kwargs\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pydantic\\v1\\main.py:341\u001b[0m, in \u001b[0;36mBaseModel.__init__\u001b[1;34m(__pydantic_self__, **data)\u001b[0m\n\u001b[0;32m    339\u001b[0m values, fields_set, validation_error \u001b[38;5;241m=\u001b[39m validate_model(__pydantic_self__\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m, data)\n\u001b[0;32m    340\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m validation_error:\n\u001b[1;32m--> 341\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m validation_error\n\u001b[0;32m    342\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    343\u001b[0m     object_setattr(__pydantic_self__, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__dict__\u001b[39m\u001b[38;5;124m'\u001b[39m, values)\n",
            "\u001b[1;31mValidationError\u001b[0m: 1 validation error for CTransformers\n__root__\n  Model path '/content/drive/MyDrive/llama-2-13b.ggmlv3.q2_K.bin' doesn't exist. (type=value_error)"
          ]
        }
      ],
      "source": [
        "#loading downloaded llm suitable for local usage,temperature(entropy/randomness in answer):1e-2\n",
        "# But first you would need to mount your Google Drive to Google Colab.\n",
        "llm = CTransformers(model=r\"/content/drive/MyDrive/llama-2-13b.ggmlv3.q2_K.bin\", model_type=\"llama\", streaming=True,\n",
        "                    callbacks=[StreamingStdOutCallbackHandler()],\n",
        "                    config={'max_new_tokens':4096,'temperature':0.01, 'context_length':4096})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WmWPAGnY7UW7"
      },
      "outputs": [],
      "source": [
        "def load_documents(file_path):\n",
        "    # Assuming you have a function to load documents (e.g., PyPDFLoader)\n",
        "    loader = PyPDFLoader(file_path)\n",
        "    docs = loader.load()\n",
        "    return docs\n",
        "\n",
        "def summary_generation(file_path):\n",
        "    # Load documents\n",
        "    docs = load_documents(file_path)\n",
        "\n",
        "    # Map\n",
        "    map_template = \"\"\"The following is a set of documents\n",
        "    {docs}\n",
        "    Based on this list of docs, please identify the main themes and concepts\n",
        "    Expand the description of each topic and concept for 2-3 lines that should include its basic descriptions, key points, and formulas if any.\n",
        "    Helpful Answer:\"\"\"\n",
        "    map_prompt = PromptTemplate.from_template(map_template)\n",
        "    map_chain = LLMChain(llm=llm, prompt=map_prompt)\n",
        "\n",
        "    # Reduce\n",
        "    reduce_template = \"\"\"The following is a set of summaries:\n",
        "    {docs}\n",
        "    Take these and distill it into a final, consolidated summary of the main topics and concepts that should include definitions and formulas of the concepts. Mention all the key points and formulas related to a concept.\n",
        "    Expand the description of each topic and concept for 2-3 lines.\n",
        "    Helpful Answer:\"\"\"\n",
        "    reduce_prompt = PromptTemplate.from_template(reduce_template)\n",
        "\n",
        "    # Run chain\n",
        "    reduce_chain = LLMChain(llm=llm, prompt=reduce_prompt)\n",
        "\n",
        "    # Takes a list of documents, combines them into a single string, and passes this to an LLMChain\n",
        "    combine_documents_chain = StuffDocumentsChain(\n",
        "        llm_chain=reduce_chain, document_variable_name=\"docs\"\n",
        "    )\n",
        "\n",
        "    # Combines and iteratively reduces the mapped documents\n",
        "    reduce_documents_chain = ReduceDocumentsChain(\n",
        "        # This is the final chain that is called.\n",
        "        combine_documents_chain=combine_documents_chain,\n",
        "        # If documents exceed context for `StuffDocumentsChain`\n",
        "        collapse_documents_chain=combine_documents_chain,\n",
        "        # The maximum number of tokens to group documents into.\n",
        "        token_max=4000,\n",
        "    )\n",
        "\n",
        "    # Combining documents by mapping a chain over them, then combining results\n",
        "    map_reduce_chain = MapReduceDocumentsChain(\n",
        "        # Map chain\n",
        "        llm_chain=map_chain,\n",
        "        # Reduce chain\n",
        "        reduce_documents_chain=reduce_documents_chain,\n",
        "        # The variable name in the llm_chain to put the documents in\n",
        "        document_variable_name=\"docs\",\n",
        "        # Return the results of the map steps in the output\n",
        "        return_intermediate_steps=False,\n",
        "    )\n",
        "\n",
        "    text_splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
        "        chunk_size=500, chunk_overlap=50\n",
        "    )\n",
        "    split_docs = text_splitter.split_documents(docs)\n",
        "    all_summaries = map_reduce_chain.run(split_docs)\n",
        "    print(all_summaries)\n",
        "\n",
        "    return all_summaries\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "all_summaries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 949
        },
        "id": "4Niho_c2MKtR",
        "outputId": "cc9d395a-33fa-449a-8611-1f5c18c0e530"
      },
      "outputs": [],
      "source": [
        "summary = summary_generation(r\"C:\\Users\\MSHOME\\Desktop\\Newfolder\\temp\\inheritance-project\\transcript_output.pdf\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FaUopfXGsblT"
      },
      "source": [
        "-------------------------------------------------------------------------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mb23GY8RWBHt",
        "outputId": "bcacc693-ac32-4bc9-8f3d-bc510d249ede"
      },
      "outputs": [],
      "source": [
        "!pip install fpdf\n",
        "!pip install pytube\n",
        "!pip install youtube-transcript-api\n",
        "!pip install fpdf\n",
        "!pip install youtube_transcript_api"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MWOG3H7rV_o8",
        "outputId": "6249cca9-d320-4f42-b294-d19ad10b8c23"
      },
      "outputs": [],
      "source": [
        "from youtube_transcript_api import YouTubeTranscriptApi\n",
        "from fpdf import *\n",
        "\n",
        "def get_transcript(youtube_url, output_pdf_path):\n",
        "    video_id = youtube_url.split(\"v=\")[-1]\n",
        "    transcript_list = YouTubeTranscriptApi.list_transcripts(video_id)\n",
        "\n",
        "    # Try fetching the manual transcript\n",
        "    try:\n",
        "        transcript = transcript_list.find_manually_created_transcript()\n",
        "        language_code = transcript.language_code  # Save the detected language\n",
        "    except:\n",
        "        # If no manual transcript is found, try fetching an auto-generated transcript in a supported language\n",
        "        try:\n",
        "            generated_transcripts = [trans for trans in transcript_list if trans.is_generated]\n",
        "            transcript = generated_transcripts[0]\n",
        "            language_code = transcript.language_code  # Save the detected language\n",
        "        except:\n",
        "            # If no auto-generated transcript is found, raise an exception\n",
        "            raise Exception(\"No suitable transcript found.\")\n",
        "\n",
        "    full_transcript = \" \".join([part['text'] for part in transcript.fetch()])\n",
        "\n",
        "    # Save the transcript to a PDF file\n",
        "    save_to_pdf(full_transcript, language_code, output_pdf_path)\n",
        "\n",
        "    return full_transcript, language_code  # Return both the transcript and detected language\n",
        "\n",
        "def save_to_pdf(transcript, language_code, output_pdf_path):\n",
        "    pdf = FPDF()\n",
        "    pdf.add_page()\n",
        "    pdf.set_font(\"Arial\", size=12)\n",
        "\n",
        "    pdf.cell(200, 10, txt=f\"Language Code: {language_code}\", ln=True, align='C')\n",
        "    pdf.ln(10)\n",
        "\n",
        "    pdf.multi_cell(0, 10, txt=transcript)\n",
        "\n",
        "    pdf.output(output_pdf_path)\n",
        "\n",
        "# Example usage\n",
        "output_pdf_path = \"transcript_output.pdf\"\n",
        "get_transcript(\"https://www.youtube.com/watch?v=LXkO4HdQXdo\", output_pdf_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xVfhPW2xw2ds",
        "outputId": "2244a5d9-791f-4e37-c7fb-64a9124fbbbe"
      },
      "outputs": [],
      "source": [
        "!pip install nltk gensim spacy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uh-S0wvow58v",
        "outputId": "8e518f6c-850e-48c5-cc8f-1dffe1b58fd2"
      },
      "outputs": [],
      "source": [
        "!python -m spacy download en_core_web_sm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gDhpLW2LxG2f"
      },
      "outputs": [],
      "source": [
        "import PyPDF2\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    with open(pdf_path, 'rb') as file:\n",
        "        pdf_reader = PyPDF2.PdfReader(file)\n",
        "        text = ''\n",
        "        for page_num in range(len(pdf_reader.pages)):\n",
        "            page = pdf_reader.pages[page_num]\n",
        "            text += page.extract_text()\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GSXDaKY_xoJ2",
        "outputId": "4fca8aa8-c4f4-4e2b-f02e-656b8a55ffd1"
      },
      "outputs": [],
      "source": [
        "!pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M9bl9esexkZz",
        "outputId": "3d0181f5-c827-4621-c639-11fe0fda374f"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "def preprocess_text(text):\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    word_tokens = word_tokenize(text)\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    preprocessed_text = [lemmatizer.lemmatize(word.lower()) for word in word_tokens if word.isalnum() and word.lower() not in stop_words]\n",
        "    return preprocessed_text\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lhHynIfExxcX"
      },
      "outputs": [],
      "source": [
        "from gensim import corpora, models\n",
        "\n",
        "def perform_lda(preprocessed_text, num_topics=5):\n",
        "    dictionary = corpora.Dictionary([preprocessed_text])\n",
        "    corpus = [dictionary.doc2bow(preprocessed_text)]\n",
        "    lda_model = models.LdaModel(corpus, num_topics=num_topics, id2word=dictionary)\n",
        "    topics = lda_model.print_topics()\n",
        "    return topics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q-htjVX3x01n",
        "outputId": "9c0e8621-ceab-4884-fc2d-510bbd0d1b11"
      },
      "outputs": [],
      "source": [
        "pdf_path = r\"/content/transcript_output.pdf\"\n",
        "text = extract_text_from_pdf(pdf_path)\n",
        "preprocessed_text = preprocess_text(text)\n",
        "topics = perform_lda(preprocessed_text)\n",
        "print(topics)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# BERT FINE-TUNED"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import TFAutoModel, AutoTokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = TFAutoModel.from_pretrained('bert-base-uncased')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "input = tokenizer(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install rouge"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from rouge import Rouge"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Your summaries and text\n",
        "# Create Rouge object\n",
        "rouge = Rouge()\n",
        "\n",
        "# Calculate ROUGE scores\n",
        "scores = rouge.get_scores(summary, text)\n",
        "\n",
        "# Print ROUGE scores\n",
        "print(scores)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def calculate_rouge_score(reference, hypothesis):\n",
        "    rouge = Rouge()\n",
        "    scores = rouge.get_scores(hypothesis, reference)\n",
        "    rouge_f1 = scores[0]['rouge-1']['p']\n",
        "    return rouge_f1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "rouge_score = calculate_rouge_score(text, summary)\n",
        "print(f\"ROUGE-N Precision Score: {rouge_score}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
